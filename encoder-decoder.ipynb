{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f610004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd1f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, causal=False):\n",
    "    super().__init__()\n",
    "\n",
    "    # Assume d_v = d_k\n",
    "    self.d_k = d_k\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "    self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "    self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "\n",
    "    # final linear layer\n",
    "    self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "\n",
    "    # causal mask\n",
    "    # make it so that diagonal is 0 too\n",
    "    # this way we don't have to shift the inputs to make targets\n",
    "    self.causal = causal\n",
    "    if causal:\n",
    "      cm = torch.tril(torch.ones(max_len, max_len))\n",
    "      self.register_buffer(\n",
    "          \"causal_mask\",\n",
    "          cm.view(1, 1, max_len, max_len)\n",
    "      )\n",
    "\n",
    "  def forward(self, q, k, v, pad_mask=None):\n",
    "    q = self.query(q) # N x T x (hd_k)\n",
    "    k = self.key(k)   # N x T x (hd_k)\n",
    "    v = self.value(v) # N x T x (hd_v)\n",
    "\n",
    "    N = q.shape[0]\n",
    "    T_output = q.shape[1]\n",
    "    T_input = k.shape[1]\n",
    "\n",
    "    # change the shape to:\n",
    "    # (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "    # in order for matrix multiply to work properly\n",
    "    q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    # compute attention weights\n",
    "    # (N, h, T, d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
    "    attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "    if pad_mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill(\n",
    "          pad_mask[:, None, None, :] == 0, float('-inf'))\n",
    "    if self.causal:\n",
    "      attn_scores = attn_scores.masked_fill(\n",
    "          self.causal_mask[:, :, :T_output, :T_input] == 0, float('-inf'))\n",
    "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "    # compute attention-weighted values\n",
    "    # (N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
    "    A = attn_weights @ v\n",
    "\n",
    "    # reshape it back before final linear layer\n",
    "    A = A.transpose(1, 2) # (N, T, h, d_k)\n",
    "    A = A.contiguous().view(N, T_output, self.d_k * self.n_heads) # (N, T, h*d_k)\n",
    "\n",
    "    # projection\n",
    "    return self.fc(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782d2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "    self.ann = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model * 4, d_model),\n",
    "        nn.Dropout(dropout_prob),\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "  \n",
    "  def forward(self, x, pad_mask=None):\n",
    "    x = self.ln1(x + self.mha(x, x, x, pad_mask))\n",
    "    x = self.ln2(x + self.ann(x))\n",
    "    x = self.dropout(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66cf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.ln3 = nn.LayerNorm(d_model)\n",
    "    self.mha1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=True)\n",
    "    self.mha2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "    self.ann = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model * 4, d_model),\n",
    "        nn.Dropout(dropout_prob),\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "  \n",
    "  def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "    # self-attention on decoder input\n",
    "    x = self.ln1(\n",
    "        dec_input + self.mha1(dec_input, dec_input, dec_input, dec_mask))\n",
    "\n",
    "    # multi-head attention including encoder output\n",
    "    x = self.ln2(x + self.mha2(x, enc_output, enc_output, enc_mask))\n",
    "\n",
    "    x = self.ln3(x + self.ann(x))\n",
    "    x = self.dropout(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022fc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    exp_term = torch.arange(0, d_model, 2)\n",
    "    div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(1, max_len, d_model)\n",
    "    pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "    pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x.shape: N x T x D\n",
    "    x = x + self.pe[:, :x.size(1), :]\n",
    "    return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604749b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               max_len,\n",
    "               d_k,\n",
    "               d_model,\n",
    "               n_heads,\n",
    "               n_layers,\n",
    "              #  n_classes,\n",
    "               dropout_prob):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "    transformer_blocks = [\n",
    "        EncoderBlock(\n",
    "            d_k,\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            max_len,\n",
    "            dropout_prob) for _ in range(n_layers)]\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "    # self.fc = nn.Linear(d_model, n_classes)\n",
    "  \n",
    "  def forward(self, x, pad_mask=None):\n",
    "    x = self.embedding(x)\n",
    "    x = self.pos_encoding(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block(x, pad_mask)\n",
    "\n",
    "    # many-to-one (x has the shape N x T x D)\n",
    "    # x = x[:, 0, :]\n",
    "\n",
    "    x = self.ln(x)\n",
    "    # x = self.fc(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2ecc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               max_len,\n",
    "               d_k,\n",
    "               d_model,\n",
    "               n_heads,\n",
    "               n_layers,\n",
    "               dropout_prob):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "    transformer_blocks = [\n",
    "        DecoderBlock(\n",
    "            d_k,\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            max_len,\n",
    "            dropout_prob) for _ in range(n_layers)]\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "    self.fc = nn.Linear(d_model, vocab_size)\n",
    "  \n",
    "  def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "    x = self.embedding(dec_input)\n",
    "    x = self.pos_encoding(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block(enc_output, x, enc_mask, dec_mask)\n",
    "    x = self.ln(x)\n",
    "    x = self.fc(x) # many-to-many\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2662a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "  \n",
    "  def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
    "    enc_output = self.encoder(enc_input, enc_mask)\n",
    "    dec_output = self.decoder(enc_output, dec_input, enc_mask, dec_mask)\n",
    "    return dec_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462ce122",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=20_000,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "decoder = Decoder(vocab_size=10_000,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "transformer = Transformer(encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c7236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(10000, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b24a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 10000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe = np.random.randint(0, 20_000, size=(8, 512))\n",
    "xe_t = torch.tensor(xe).to(device)\n",
    "\n",
    "xd = np.random.randint(0, 10_000, size=(8, 256))\n",
    "xd_t = torch.tensor(xd).to(device)\n",
    "\n",
    "maske = np.ones((8, 512))\n",
    "maske[:, 256:] = 0\n",
    "maske_t = torch.tensor(maske).to(device)\n",
    "\n",
    "maskd = np.ones((8, 256))\n",
    "maskd[:, 128:] = 0\n",
    "maskd_t = torch.tensor(maskd).to(device)\n",
    "\n",
    "out = transformer(xe_t, xd_t, maske_t, maskd_t)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d07a0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0838,  0.4689, -0.8988,  ..., -0.7771, -1.2173, -0.3510],\n",
       "         [-1.3486,  0.1689,  0.5149,  ..., -0.5438, -1.0344, -0.8729],\n",
       "         [-0.5519, -1.0766, -0.0190,  ...,  0.1202,  0.0863, -0.5699],\n",
       "         ...,\n",
       "         [-0.7001, -0.6699, -0.3183,  ...,  0.1169, -0.0726, -0.1699],\n",
       "         [ 0.1086, -0.8313, -0.0122,  ...,  0.2362, -0.0886,  0.7051],\n",
       "         [ 0.0644,  0.6151, -1.3603,  ...,  0.7313, -0.3213,  1.4813]],\n",
       "\n",
       "        [[-0.0974,  1.6530, -0.6403,  ..., -0.4491, -0.8697,  0.5653],\n",
       "         [-0.2173,  0.4646, -0.6453,  ..., -0.1123, -0.2016,  0.5505],\n",
       "         [ 0.3307,  0.5438, -0.5280,  ..., -0.3066, -0.3113,  0.0534],\n",
       "         ...,\n",
       "         [ 1.0794, -0.0807, -1.2905,  ...,  0.8979,  0.3465,  0.5867],\n",
       "         [-0.2698,  0.1874, -0.6488,  ...,  1.1859,  1.0347,  0.4326],\n",
       "         [ 0.3200,  0.2562, -0.4384,  ...,  0.2891, -0.5363,  0.5783]],\n",
       "\n",
       "        [[-0.7660,  0.3287, -0.3151,  ...,  0.3258, -0.8193,  0.1250],\n",
       "         [ 0.6688,  1.7092,  0.4422,  ...,  0.0237, -0.7176, -0.6075],\n",
       "         [-0.4421, -0.3568,  0.6233,  ..., -0.2628,  0.2415,  0.0075],\n",
       "         ...,\n",
       "         [ 1.0745, -0.9678, -1.5190,  ...,  0.6608,  0.7260,  0.3468],\n",
       "         [ 1.3269,  0.2133, -1.5561,  ...,  0.6456,  0.0978,  0.5062],\n",
       "         [ 0.4442, -0.1685, -1.2500,  ...,  0.5765,  0.5499,  1.0469]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5713,  0.9706, -0.3814,  ...,  0.2292,  0.1607, -0.2568],\n",
       "         [-0.3211,  0.5277,  0.6393,  ...,  0.2889, -0.3953,  0.1531],\n",
       "         [-0.4014,  0.3139,  0.9672,  ..., -0.2478, -0.5115, -0.9290],\n",
       "         ...,\n",
       "         [ 0.1499, -0.3497, -0.6947,  ...,  0.7644,  0.2091,  0.5007],\n",
       "         [ 0.5291,  0.1534, -0.4164,  ...,  0.7759,  0.2808,  0.2694],\n",
       "         [-0.3190, -0.4271, -0.6283,  ..., -0.1313,  0.1436,  0.1048]],\n",
       "\n",
       "        [[ 1.1290,  0.3134, -0.3352,  ..., -0.1020, -0.5712,  0.0320],\n",
       "         [ 0.8916,  0.7774, -0.6400,  ...,  0.4947, -0.5469, -0.0540],\n",
       "         [ 0.0359,  0.1939,  0.6566,  ..., -0.1457, -0.3544, -0.6052],\n",
       "         ...,\n",
       "         [ 0.0092, -0.1459, -0.5709,  ...,  0.7908,  0.0766,  0.6984],\n",
       "         [ 0.5426, -0.2212, -0.8255,  ...,  0.8628,  1.1410,  0.2351],\n",
       "         [ 1.8069,  0.3883, -0.3911,  ..., -0.1832,  0.7454,  0.6357]],\n",
       "\n",
       "        [[ 0.8565,  1.0962, -0.5527,  ..., -0.1181, -0.2575, -0.6274],\n",
       "         [ 0.8095,  0.7351, -1.2967,  ...,  0.2550, -0.3659, -0.0353],\n",
       "         [ 0.2637,  0.4694, -0.5979,  ..., -0.4396, -0.3350,  0.5538],\n",
       "         ...,\n",
       "         [ 0.6632,  0.4505, -0.1889,  ..., -0.3060,  0.1806,  0.9923],\n",
       "         [ 0.6389,  0.7684, -0.7778,  ...,  0.7308,  0.6266, -0.0081],\n",
       "         [ 0.2695, -0.0999, -1.9212,  ...,  1.0008,  0.9281,  0.6224]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9060f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-08-04 17:16:08--  https://lazyprogrammer.me/course_files/nlp3/spa.txt\n",
      "Resolving lazyprogrammer.me (lazyprogrammer.me)... 172.64.80.1\n",
      "Connecting to lazyprogrammer.me (lazyprogrammer.me)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/plain]\n",
      "Saving to: 'spa.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 7.57M\n",
      "    50K .......... .......... .......... .......... ..........  445K\n",
      "   100K .......... .......... .......... .......... ..........  383K\n",
      "   150K .......... .......... .......... .......... .......... 20.1M\n",
      "   200K .......... .......... .......... .......... .......... 13.9M\n",
      "   250K .......... .......... .......... .......... ..........  430K\n",
      "   300K .......... .......... .......... .......... .......... 10.8M\n",
      "   350K .......... .......... .......... .......... ..........  422K\n",
      "   400K .......... .......... .......... .......... ..........  234M\n",
      "   450K .......... .......... .......... .......... .......... 38.5M\n",
      "   500K .......... .......... .......... .......... ..........  272K\n",
      "   550K .......... .......... .......... .......... .......... 33.6M\n",
      "   600K .......... .......... .......... .......... ..........  243M\n",
      "   650K .......... .......... .......... .......... .......... 44.3M\n",
      "   700K .......... .......... .......... .......... ..........  833K\n",
      "   750K .......... .......... .......... .......... ..........  157K\n",
      "   800K .......... .......... .......... .......... .......... 85.3M\n",
      "   850K .......... .......... .......... .......... ..........  569M\n",
      "   900K .......... .......... .......... .......... ..........  751M\n",
      "   950K .......... .......... .......... .......... ..........  380M\n",
      "  1000K .......... .......... .......... .......... ..........  576M\n",
      "  1050K .......... .......... .......... .......... ..........  928M\n",
      "  1100K .......... .......... .......... .......... ..........  566M\n",
      "  1150K .......... .......... .......... .......... ..........  536M\n",
      "  1200K .......... .......... .......... .......... ..........  589K\n",
      "  1250K .......... .......... .......... .......... ..........  555K\n",
      "  1300K .......... .......... .......... .......... ..........  696M\n",
      "  1350K .......... .......... .......... .......... .......... 40.1M\n",
      "  1400K .......... .......... .......... .......... .......... 20.6M\n",
      "  1450K .......... .......... .......... .......... ..........  179M\n",
      "  1500K .......... .......... .......... .......... ..........  470M\n",
      "  1550K .......... .......... .......... .......... ..........  871K\n",
      "  1600K .......... .......... .......... .......... .......... 32.8M\n",
      "  1650K .......... .......... .......... .......... ..........  822K\n",
      "  1700K .......... .......... .......... .......... .......... 50.5M\n",
      "  1750K .......... .......... .......... .......... ..........  833K\n",
      "  1800K .......... .......... .......... .......... .......... 31.2M\n",
      "  1850K .......... .......... .......... .......... ..........  428M\n",
      "  1900K .......... .......... .......... .......... ..........  846K\n",
      "  1950K .......... .......... .......... .......... .......... 28.9M\n",
      "  2000K .......... .......... .......... .......... ..........  792K\n",
      "  2050K .......... .......... .......... .......... ..........  198M\n",
      "  2100K .......... .......... .......... .......... .......... 55.6M\n",
      "  2150K .......... .......... .......... .......... ..........  835K\n",
      "  2200K .......... .......... .......... .......... ..........  241M\n",
      "  2250K .......... .......... .......... .......... .......... 45.4M\n",
      "  2300K .......... .......... .......... .......... ..........  817K\n",
      "  2350K .......... .......... .......... .......... ..........  197M\n",
      "  2400K .......... .......... .......... .......... .......... 47.6M\n",
      "  2450K .......... .......... .......... .......... ..........  121M\n",
      "  2500K .......... .......... .......... .......... ..........  500K\n",
      "  2550K .......... .......... .......... .......... ..........  289M\n",
      "  2600K .......... .......... .......... .......... ..........  232M\n",
      "  2650K .......... .......... .......... .......... ..........  342M\n",
      "  2700K .......... .......... .......... .......... .......... 2.14M\n",
      "  2750K .......... .......... .......... .......... ..........  110M\n",
      "  2800K .......... .......... .......... .......... ..........  763K\n",
      "  2850K .......... .......... .......... .......... ..........  137M\n",
      "  2900K .......... .......... .......... .......... ..........  756K\n",
      "  2950K .......... .......... .......... .......... .......... 57.1M\n",
      "  3000K .......... .......... .......... .......... .......... 29.4M\n",
      "  3050K .......... .......... .......... .......... ..........  157M\n",
      "  3100K .......... .......... .......... .......... ..........  155M\n",
      "  3150K .......... .......... .......... .......... .......... 1016K\n",
      "  3200K .......... .......... .......... .......... ..........  228M\n",
      "  3250K .......... .......... .......... .......... .......... 54.1M\n",
      "  3300K .......... .......... .......... .......... ..........  807K\n",
      "  3350K .......... .......... .......... .......... ..........  298M\n",
      "  3400K .......... .......... .......... .......... .......... 28.7M\n",
      "  3450K .......... .......... .......... .......... .......... 39.9M\n",
      "  3500K .......... .......... .......... .......... ..........  145M\n",
      "  3550K .......... .......... .......... .......... ..........  676K\n",
      "  3600K .......... .......... .......... .......... .......... 22.3M\n",
      "  3650K .......... .......... .......... .......... ..........  540M\n",
      "  3700K .......... .......... .......... .......... .......... 1.08M\n",
      "  3750K .......... .......... .......... .......... .......... 36.0M\n",
      "  3800K .......... .......... .......... .......... ..........  497M\n",
      "  3850K .......... .......... .......... .......... .......... 38.8M\n",
      "  3900K .......... .......... .......... .......... ..........  572M\n",
      "  3950K .......... .......... .......... .......... .......... 27.1M\n",
      "  4000K .......... .......... .......... .......... ..........  919K\n",
      "  4050K .......... .......... .......... .......... .......... 12.9M\n",
      "  4100K .......... .......... .......... .......... ..........  479M\n",
      "  4150K .......... .......... .......... .......... .......... 56.5M\n",
      "  4200K .......... .......... .......... .......... ..........  839K\n",
      "  4250K .......... .......... .......... .......... ..........  722M\n",
      "  4300K .......... .......... .......... .......... .......... 32.7M\n",
      "  4350K .......... .......... .......... .......... .......... 24.0M\n",
      "  4400K .......... .......... .......... .......... ..........  368M\n",
      "  4450K .......... .......... .......... .......... ..........  876K\n",
      "  4500K .......... .......... .......... .......... ..........  344M\n",
      "  4550K .......... .......... .......... .......... .......... 21.9M\n",
      "  4600K .......... .......... .......... .......... .......... 72.2M\n",
      "  4650K .......... .......... .......... .......... .......... 50.6M\n",
      "  4700K .......... .......... .......... .......... ..........  561M\n",
      "  4750K .......... .......... .......... .......... ..........  859K\n",
      "  4800K .......... .......... .......... .......... .......... 31.0M\n",
      "  4850K .......... .......... .......... .......... ..........  359M\n",
      "  4900K .......... .......... .......... .......... .......... 47.2M\n",
      "  4950K .......... .......... .......... .......... ..........  855K\n",
      "  5000K .......... .......... .......... .......... .......... 33.3M\n",
      "  5050K .......... .......... .......... .......... ..........  765M\n",
      "  5100K .......... .......... .......... .......... .......... 14.5M\n",
      "  5150K .......... .......... .......... .......... ..........  390M\n",
      "  5200K .......... .......... .......... .......... ..........  538M\n",
      "  5250K .......... .......... .......... .......... ..........  600K\n",
      "  5300K .......... .......... .......... .......... ..........  258M\n",
      "  5350K .......... .......... .......... .......... .......... 31.5M\n",
      "  5400K .......... .......... .......... .......... ..........  388M\n",
      "  5450K .......... .......... .......... .......... .......... 50.9M\n",
      "  5500K .......... .......... .......... .......... .......... 4.02M\n",
      "  5550K .......... .......... .......... .......... .......... 38.6M\n",
      "  5600K .......... .......... .......... .......... .......... 2.37M\n",
      "  5650K .......... .......... .......... .......... ..........  136M\n",
      "  5700K .......... .......... .......... .......... .......... 20.2M\n",
      "  5750K .......... .......... .......... .......... .......... 1.31M\n",
      "  5800K .......... .......... .......... .......... .......... 26.6M\n",
      "  5850K .......... .......... .......... .......... .......... 23.1M\n",
      "  5900K .......... .......... .......... .......... .......... 2.90M\n",
      "  5950K .......... .......... .......... .......... ..........  279M\n",
      "  6000K .......... .......... .......... .......... .......... 34.7M\n",
      "  6050K .......... .......... .......... .......... .......... 25.2M\n",
      "  6100K .......... .......... .......... .......... ..........  571M\n",
      "  6150K .......... .......... .......... .......... ..........  858K\n",
      "  6200K .......... .......... .......... .......... ..........  326M\n",
      "  6250K .......... .......... .......... .......... .......... 34.1M\n",
      "  6300K .......... .......... .......... .......... ..........  873M\n",
      "  6350K .......... .......... .......... .......... ..........  811K\n",
      "  6400K .......... .......... .......... .......... .......... 61.1M\n",
      "  6450K .......... .......... .......... .......... .......... 11.0M\n",
      "  6500K .......... .......... .......... .......... .......... 5.92M\n",
      "  6550K .......... .......... .......... .......... ..........  276M\n",
      "  6600K .......... .......... .......... .......... ..........  878M\n",
      "  6650K .......... .......... .......... .......... ..........  724M\n",
      "  6700K .......... .......... .......... .......... ..........  303M\n",
      "  6750K .......... .......... .......... .......... ..........  557M\n",
      "  6800K .......... .......... .......... .......... .......... 85.1M\n",
      "  6850K .......... .......... .......... .......... .......... 2.01M\n",
      "  6900K .......... .......... .......... .......... .......... 1.90M\n",
      "  6950K .......... .......... .......... .......... .......... 46.4M\n",
      "  7000K .......... .......... .......... .......... ..........  559M\n",
      "  7050K .......... .......... .......... .......... .......... 35.0M\n",
      "  7100K .......... .......... .......... .......... ..........  843M\n",
      "  7150K .......... .......... .......... .......... .......... 31.9M\n",
      "  7200K .......... .......... .......... .......... ..........  916K\n",
      "  7250K .......... .......... .......... .......... .......... 41.3M\n",
      "  7300K .......... .......... .......... .......... ..........  705M\n",
      "  7350K .......... .......... .......... .......... .......... 8.98M\n",
      "  7400K .......... .......... .......... .......... ..........  502M\n",
      "  7450K .......... .......... .......... .......... .......... 35.0M\n",
      "  7500K .......... .......... .......... .......... ..........  716M\n",
      "  7550K .......... .......... .......... .......... ..........  939K\n",
      "  7600K .......... .......... .......... ...                    276M=3.0s\n",
      "\n",
      "2025-08-04 17:16:12 (2.51 MB/s) - 'spa.txt' saved [7817148]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://lazyprogrammer.me/course_files/nlp3/spa.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "944c0159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0        1\n",
       "0   Go.      Ve.\n",
       "1   Go.    Vete.\n",
       "2   Go.    Vaya.\n",
       "3   Hi.    Hola.\n",
       "4  Run!  ¡Corre!"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spa.txt', sep=\"\\t\", header=None)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bc7b4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "438bdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:30_000] # takes too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ddd4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['en', 'es']\n",
    "df.to_csv('spa.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3f5179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Emil\\anaconda3\\envs\\transformer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 0 examples [00:00, ? examples/s]c:\\Users\\Emil\\anaconda3\\envs\\transformer\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating train split: 30000 examples [00:00, 426442.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset('csv', data_files='spa.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da78728a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cf66dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 21000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = raw_dataset['train'].train_test_split(test_size=0.3, seed=42)\n",
    "split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38ecb0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Emil\\anaconda3\\envs\\transformer\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8f97f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Yo', '▁puedo', '▁arreglarlo', '.', '</s>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split[\"train\"][0][\"en\"]\n",
    "es_sentence = split[\"train\"][0][\"es\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "targets = tokenizer(text_target=es_sentence)\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(targets['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ddf877d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yo puedo arreglarlo.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e299d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch['en'], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    labels = tokenizer(\n",
    "        text_target=batch['es'], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b93d050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 21000/21000 [00:02<00:00, 7118.80 examples/s]\n",
      "Map: 100%|██████████| 9000/9000 [00:01<00:00, 6751.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = split.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d795e760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7571cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f100a7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[   33,    88,  9222,    48,     3,     0, 65000, 65000],\n",
       "        [  552, 11490,     9,   310,   255,     3,     0, 65000],\n",
       "        [  143,    31,   125,  1208,     3,     0, 65000, 65000],\n",
       "        [ 1093,   220,  1890,    23,    48,     3,     0, 65000],\n",
       "        [  124,    20,   100, 18422,    48,   141,     3,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  711,  1039, 44159,     3,     0,  -100,  -100,  -100],\n",
       "        [ 2722, 18663,   239,   212,     3,     0,  -100,  -100],\n",
       "        [  539,    43,   155,   960,     3,     0,  -100,  -100],\n",
       "        [15165,  1250,   380,  3564,    36,  1016,     3,     0],\n",
       "        [  350,     8, 19153,    29, 31326,     3,     0,  -100]])})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(0, 5)])\n",
    "batch.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "826776f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   33,    88,  9222,    48,     3,     0, 65000, 65000],\n",
       "        [  552, 11490,     9,   310,   255,     3,     0, 65000],\n",
       "        [  143,    31,   125,  1208,     3,     0, 65000, 65000],\n",
       "        [ 1093,   220,  1890,    23,    48,     3,     0, 65000],\n",
       "        [  124,    20,   100, 18422,    48,   141,     3,     0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6f7f2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5e9ced3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  711,  1039, 44159,     3,     0,  -100,  -100,  -100],\n",
       "        [ 2722, 18663,   239,   212,     3,     0,  -100,  -100],\n",
       "        [  539,    43,   155,   960,     3,     0,  -100,  -100],\n",
       "        [15165,  1250,   380,  3564,    36,  1016,     3,     0],\n",
       "        [  350,     8, 19153,    29, 31326,     3,     0,  -100]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34bd3711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 65000]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31531178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', '<unk>', '<pad>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df974259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [65000, 0], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('<pad>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bdd955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37b1bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: input_ids v.shape: torch.Size([32, 9])\n",
      "k: attention_mask v.shape: torch.Size([32, 9])\n",
      "k: labels v.shape: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "# check how it works\n",
    "for batch in train_loader:\n",
    "  for k, v in batch.items():\n",
    "    print(\"k:\", k, \"v.shape:\", v.shape)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b6e2d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65001"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b46ada16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ѕэр'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([60000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f4374b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ѕэр\n",
      "֖\n",
      "אהוד\n",
      "אמת\n",
      "ישראל\n",
      "שאול\n",
      "؋\n",
      "أ،\n",
      "أحد\n",
      "اسیدپاشی\n",
      "الأحرار\n",
      "الارض\n",
      "الدول\n",
      "الشعب\n",
      "العرب\n",
      "اله\n",
      "الوطني\n",
      "بازخوری\n",
      "بالموئل\n",
      "بصمت\n",
      "دونم\n",
      "رئيسية\n",
      "ريم\n",
      "سورة\n",
      "سوريا\n",
      "سکائی\n",
      "سید\n",
      "شریف\n",
      "صالح\n",
      "ضد\n",
      "طلال\n",
      "عبيد\n",
      "فضای\n",
      "قاموس\n",
      "قبل\n",
      "لبيك\n",
      "للحجب\n",
      "لله\n",
      "متعلقة\n",
      "مثلي\n",
      "محسن\n",
      "مدني\n",
      "مسجد\n",
      "مسعود\n",
      "مصطلحات\n",
      "معنية\n",
      "مواضيع\n",
      "ناصر\n",
      "نزار\n",
      "نواز\n",
      "هل\n",
      "هيثم\n",
      "وادي\n",
      "پرتیبھا\n",
      "پکیج\n",
      "چین\n",
      "کم\n",
      "বিজয়\n",
      "ਭਾਰਤ\n",
      "คิง\n",
      "ེ\n",
      "ဪဪ\n",
      "ᄆ\n",
      "ᠱ\n",
      "ᠶ\n",
      "ᢲ\n",
      "ᴘʀᴏ\n",
      "ṇḍ\n",
      "ṓ\n",
      "Ả\n",
      "ἀήρ\n",
      "ἐκεῖνος\n",
      "‍✈️\n",
      "’à\n",
      "††††††††\n",
      "′û\n",
      "₴\n",
      "←←←\n",
      "←→\n",
      "∎\n",
      "−∞\n",
      "∘\n",
      "√§\n",
      "√√\n",
      "∠\n",
      "∮\n",
      "⊱\n",
      "⎢\n",
      "⎼\n",
      "├Ą\n",
      "▀\n",
      "▣\n",
      "▦\n",
      "▬\n",
      "▲一\n",
      "►►►\n",
      "◂\n",
      "♦♦♦\n",
      "♪♪♪♪\n",
      "⚜\n",
      "⚪️\n",
      "⚽\n",
      "✆\n",
      "✩✩\n",
      "✯\n",
      "❄\n",
      "❶\n",
      "➍\n",
      "⦶\n",
      "もっと知る\n",
      "もっと見る\n",
      "よく\n",
      "ク\n",
      "ベド\n",
      "マップ\n",
      "三亚\n",
      "不倫\n",
      "与合作伙伴们共同开发的\n",
      "丝\n",
      "个\n",
      "久保田智広\n",
      "么\n",
      "书\n",
      "人\n",
      "什么\n",
      "作業療法\n",
      "値\n",
      "像\n",
      "军队应该允许妇女在在战斗岗位中服役吗\n",
      "到\n",
      "包子\n",
      "即时中国大陆映像\n",
      "古城\n",
      "另外\n",
      "回到顶部\n",
      "国家民族事务委员会经济发展司\n",
      "国家统计局人口和社会科技统计司\n",
      "多\n",
      "大\n",
      "天天看高清影视\n",
      "奇摩輸入法\n",
      "如\n",
      "如意淘\n",
      "学生宿舍\n",
      "它应该是非法烧我们的国旗\n",
      "当店へのアクセス\n",
      "您支持单一支付者医保系统吗\n",
      "抗原\n",
      "折角\n",
      "拼音输入法\n",
      "支持机构\n",
      "文敏\n",
      "明故宫\n",
      "更多范文\n",
      "木\n",
      "本平台是由\n",
      "松田聖子\n",
      "林\n",
      "枚\n",
      "毛泽东\n",
      "汉语\n",
      "注意\n",
      "洪門\n",
      "清\n",
      "游戏\n",
      "游泳\n",
      "烤鸭\n",
      "猴\n",
      "玄師\n",
      "电子邮箱\n",
      "米津\n",
      "納豆\n",
      "繁體\n",
      "结果\n",
      "羊肉串\n",
      "美国是否应该留在联合国\n",
      "茜\n",
      "虎\n",
      "跟着我们\n",
      "路\n",
      "这样\n",
      "重庆\n",
      "金士顿技术支持\n",
      "鈴木雅之\n",
      "钅\n",
      "钟\n",
      "长城\n",
      "陳典桓\n",
      "隐私政策\n",
      "青岛\n",
      "面\n",
      "韩文强\n",
      "頨\n",
      "饺子\n",
      "鮥\n",
      "麻辣烫\n",
      "동현\n",
      "되자\n",
      "세계\n",
      "안상홍님\n",
      "어머니와\n",
      "영생\n",
      "예루살렘\n",
      "예를\n",
      "제\n",
      "중국\n",
      "집\n",
      "찾으라\n",
      "천국\n",
      "코맨\n",
      "하나님\n",
      "한국어Č\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##########\n",
      "#^\n",
      "#Євромайдан\n",
      "#كلنا\n",
      "#†\n",
      "#社畜死亡かるた\n",
      "$###\n",
      "$±\n",
      "+£\n",
      "+»\n",
      "==<\n",
      "=Í\n",
      "=’\n",
      ">>¡\n",
      "\\\\$\n",
      "\\\\\\\\{@}\n",
      "\\Í\n",
      "^^\\\n",
      "`=\n",
      "`Ú\n",
      "{@\n",
      "~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~\n",
      "~♬\n",
      "¡Á\n",
      "¢ó\n",
      "£$ø\n",
      "¤¤\n",
      "¤ю\n",
      "§£\n",
      "«É\n",
      "«Справедливая\n",
      "«岞\n",
      "¬@\n",
      "®∞\n",
      "·■·\n",
      "»^\n",
      "Âż\n",
      "Åð\n",
      "Ì±\n",
      "Íï\n",
      "Ð£¶Ô\n",
      "Ð¥\n",
      "Ðî\n",
      "Ôï\n",
      "ÖĞÜŞ\n",
      "Üğ\n",
      "Þò\n",
      "àà\n",
      "àì\n",
      "á»\n",
      "áť\n",
      "ãîäè\n",
      "ãñá\n",
      "äÖá\n",
      "äàð\n",
      "åì\n",
      "åîáãùãÞ\n",
      "åôÞ\n",
      "åôÞó\n",
      "æå\n",
      "çè\n",
      "èï\n",
      "ìïëïã\n",
      "ìò\n",
      "ï¿\n",
      "ïè\n",
      "ïý\n",
      "ðáñ\n",
      "ðïóüóôùóç\n",
      "òò\n",
      "óç\n",
      "óôñïöÞ\n",
      "óõ\n",
      "ôçô\n",
      "ôçôá\n",
      "õô\n",
      "÷à\n",
      "÷ï\n",
      "÷òî\n",
      "üö\n",
      "ýìáôá\n",
      "ýč\n",
      "þýð\n",
      "āḍ\n",
      "Ă¡\n",
      "ğŸ\n",
      "İşç\n",
      "Ļ\n",
      "ŁĘ\n",
      "ōō\n",
      "Şə\n",
      "šķī\n",
      "šḫ\n",
      "ţī\n",
      "źć\n",
      "Žá\n",
      "žë\n",
      "žā\n",
      "ǣ\n",
      "ȏ\n",
      "ș»\n",
      "ȯ\n",
      "ɔː\n",
      "ɪŋ\n",
      "ɪˌ\n",
      "ɸ\n",
      "ʁ\n",
      "ʼʹ\n",
      "́ü\n",
      "̧¶\n",
      "Ία\n",
      "ΑΗΡ\n",
      "ΑΠ\n",
      "Αγκινάρα\n",
      "ΒΑΝΟΤΠ\n",
      "ΒΙΟ\n",
      "ΓΕΟΡ\n",
      "ΓΗ\n",
      "ΓΚΤ\n",
      "ΓΟ\n",
      "Γαλανό\n",
      "Γεώργιος\n",
      "Γν\n",
      "Γρ\n",
      "Δήμων\n",
      "Δημοσιεύθηκε\n",
      "Δημοτικές\n",
      "ΕΊ\n",
      "ΕΓ\n",
      "ΕΥΡΩ\n",
      "Ελλάς\n",
      "Ελληνική\n",
      "ΕπίσημοςΠαραλήπτης\n",
      "Θεσσαλονίκη\n",
      "Ιρίων\n",
      "ΚΑ\n",
      "Κουμανδαρία\n",
      "Κύπρου\n",
      "ΛΜ\n",
      "Λουκάνικο\n",
      "ΜΙΉ\n",
      "Μανουήλ\n",
      "Μαντινεία\n",
      "Μελεκούνι\n",
      "Μεταγγιτσίου\n",
      "Ξη\n",
      "ΟΓΑ\n",
      "ΟΙ\n",
      "Πάνω\n",
      "Παλαιολόγος\n",
      "Παφίτικο\n",
      "Πευκοθυμαρόμελο\n",
      "ΡΑ\n",
      "ΡΑΒΕ\n",
      "ΡΓ\n",
      "ΣΑ\n",
      "Σάμος\n",
      "Τράπεζα\n",
      "Υπηρεσία\n",
      "Φενεού\n",
      "Ω·\n",
      "βηίβφΓίεβ\n",
      "δ=+\n",
      "είναι\n",
      "εἰς\n",
      "ιί\n",
      "καὶ\n",
      "μέσω\n",
      "μη\n",
      "μηι\n",
      "μιη\n",
      "μου\n",
      "μπιοΐ\n",
      "σ^\n",
      "σου\n",
      "στ\n",
      "χτο\n",
      "χώρα\n",
      "Ёрсю\n",
      "Ёрсю=\n",
      "Із\n",
      "Алтучер\n",
      "Ариэли\n",
      "БАККАРА\n",
      "Бела\n",
      "Белый\n",
      "Белых\n",
      "Бурбо\n",
      "Бутакова\n",
      "Вї\n",
      "Галина\n",
      "Голодец\n",
      "Город\n",
      "Град\n",
      "День\n",
      "Димитров\n",
      "Довлатов\n",
      "Душан\n",
      "Евгений\n",
      "Загрузка\n",
      "Замяткин\n",
      "Зюганов\n",
      "КСМ\n",
      "Карлово\n",
      "Космос\n",
      "Листопад\n",
      "Ловеч\n",
      "Марвы\n",
      "Мастерица\n",
      "Милан\n",
      "Минск\n",
      "Мисхор\n",
      "Моэрмана\n",
      "Никита\n",
      "Новая\n",
      "Одесса\n",
      "Ольберса\n",
      "Опширније\n",
      "ПК\n",
      "Павликени\n",
      "Петр\n",
      "Плевен\n",
      "Простукивание\n",
      "Пхё=хЁър\n",
      "РДТ\n",
      "РСМ\n",
      "Роле\n",
      "Русе\n",
      "РярЁ\n",
      "Совет\n",
      "Стари\n",
      "Сунгурларе\n",
      "Тв\n",
      "Трансмашхолдинг\n",
      "Трапезица\n",
      "Универсал\n",
      "ФК\n",
      "Форзиция\n",
      "Франция\n",
      "ХХ\n",
      "Христо\n",
      "Цена\n",
      "Черниговская\n",
      "ЭГ\n",
      "Энн\n",
      "апреля\n",
      "ба\n",
      "бардакош\n",
      "вЂќ\n",
      "вестник\n",
      "вещий\n",
      "водяную\n",
      "году\n",
      "ее\n",
      "ес\n",
      "железы\n",
      "заведения\n",
      "июля\n",
      "квартира\n",
      "колбасе\n",
      "лайфхаков\n",
      "лечебните\n",
      "листки\n",
      "моторшоу\n",
      "настое\n",
      "наш\n",
      "но\n",
      "новости\n",
      "ответ\n",
      "партыя\n",
      "похудеете\n",
      "процеди\n",
      "размешиваем\n",
      "село\n",
      "сидератов\n",
      "сопротивляемся\n",
      "соцсетей\n",
      "сс\n",
      "субличности\n",
      "тимеросалом\n",
      "турмерик\n",
      "турмерика\n",
      "ул\n",
      "умелица\n",
      "фото\n",
      "хЁъш\n",
      "читать\n",
      "ьр\n",
      "электробус\n",
      "электромобилем\n",
      "эхь\n",
      "юъэю\n",
      "בייַ\n",
      "ברא\n",
      "גן\n",
      "הוא\n",
      "מרידור\n",
      "פסח\n",
      "קצב\n",
      "؟؟\n",
      "آزادی\n",
      "احتلال\n",
      "احمدشجاع\n",
      "از\n",
      "الأمير\n",
      "الإخوان\n",
      "الإسلامي\n",
      "الثورة\n",
      "الجوية\n",
      "العالم\n",
      "العربيـة\n",
      "الفتاح\n",
      "الكبير\n",
      "المصريين\n",
      "النهضة\n",
      "با\n",
      "بس\n",
      "به\n",
      "جزيرة\n",
      "جسر\n",
      "حتى\n",
      "دبي\n",
      "دمشق\n",
      "شاه\n",
      "شیادی\n",
      "عرض\n",
      "عشان\n",
      "غزة\n",
      "فلسطين\n",
      "فى\n",
      "كان\n",
      "لي\n",
      "مارس\n",
      "مبارك\n",
      "مدن\n",
      "مراكش\n",
      "معاذ\n",
      "نون\n",
      "وقال\n",
      "ّ\n",
      "हिंदीनेपाली\n",
      "තාපසතුමන්ට\n",
      "฀•฀฀฀\n",
      "กาแฟดอยตช้าง\n",
      "กาแฟดอยตุง\n",
      "น\n",
      "ยช\n",
      "ยบ\n",
      "ဠ\n",
      "ᠢ\n",
      "᪡\n",
      "᳣\n",
      "ᴓ\n",
      "Ḍ\n",
      "ḱ\n",
      "ṣá\n",
      "ṣī\n",
      "Ṭá\n",
      "ṭī\n",
      "ỷ\n",
      "ὄρος\n",
      "Ᾱ\n",
      "ῆ\n",
      "‘î\n",
      "’œ\n",
      "„Ś\n",
      "††††\n",
      "•Φ\n",
      "‫#\n",
      "‿◠\n",
      "€•\n",
      "₮\n",
      "∂†\n",
      "∗∗∗\n",
      "√®\n",
      "≧◡≦\n",
      "⊂\n",
      "⋮\n",
      "⌋\n",
      "╚\n",
      "●●●\n",
      "◗\n",
      "☭\n",
      "♠️\n",
      "✊\n",
      "✌️\n",
      "✴\n",
      "➎\n",
      "〉\n",
      "は\n",
      "アメリカ\n",
      "チ\n",
      "トップ\n",
      "パーカー\n",
      "七月\n",
      "上\n",
      "乙\n",
      "亳\n",
      "他\n",
      "位\n",
      "分钟\n",
      "创\n",
      "南\n",
      "博客\n",
      "厘\n",
      "吗\n",
      "咜\n",
      "問\n",
      "因为\n",
      "家\n",
      "岞\n",
      "帮助\n",
      "当然\n",
      "忙しい\n",
      "想\n",
      "或\n",
      "方法\n",
      "是否应允许公民在离岸银行账户中储蓄或抽奖\n",
      "曾\n",
      "李\n",
      "李克强\n",
      "村下孝蔵\n",
      "正月\n",
      "比\n",
      "清水\n",
      "琴\n",
      "私\n",
      "管理局\n",
      "罗\n",
      "能\n",
      "舞\n",
      "苗\n",
      "葵\n",
      "豬\n",
      "钟楼\n",
      "首页\n",
      "鬵\n",
      "鮦\n",
      "黄芪\n",
      "ꞌ\n",
      "가\n",
      "감사\n",
      "나\n",
      "남겨\n",
      "내린\n",
      "다음\n",
      "를\n",
      "사랑\n",
      "시\n",
      "신이\n",
      "을\n",
      "음식\n",
      "잘\n",
      "코멘트를\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############\n",
      "#~\n",
      "#евромайдан\n",
      "#елкамк\n",
      "#путин\n",
      "#الحرية\n",
      "#اليوم\n",
      "#جمعة\n",
      "#ضد\n",
      "#طلعت\n",
      "#وين\n",
      "$Δ\n",
      "$μ$\n",
      "+#\n",
      "+Ёрщьёр\n",
      "+Іщ\n",
      "+рссы\n",
      "+рссыр\n",
      "+с\n",
      "+фшёёх\n",
      "+хЁЁшэу\n",
      "+хэѕ¶х\n",
      "+ыхъю\n",
      "+’\n",
      "=^\n",
      "=Ёющър\n",
      ">♪\n",
      "@$\n",
      "\\ΛΛ\n",
      "^«\n",
      "^¬^\n",
      "`Ë\n",
      "`ī\n",
      "{^\n",
      "}^{\n",
      "~Ί\n",
      "~↓\n",
      "¡©\n",
      "¡»\n",
      "¤ш\n",
      "¤юёых\n",
      "¤ѕ\n",
      "¥=¥\n",
      "¦ртЁр\n",
      "¦ртЁх\n",
      "¦рурЁрё\n",
      "¦стш\n",
      "¦фшя\n",
      "¦шъ\n",
      "§юч\n",
      "«#\n",
      "«Солидарность»\n",
      "¬ø\n",
      "®Ÿ\n",
      "°$\n",
      "°>\n",
      "°¿\n",
      "°ъю\n",
      "°ъюы\n",
      "°\n",
      "¶ртэр\n",
      "¶юЁьх\n",
      "·°\n",
      "·Δ\n",
      "ÂÝê\n",
      "Â€\n",
      "Ã¢Â\n",
      "ÃŒÂ\n",
      "ÃƒÆ’Ã‚Â\n",
      "Ãˆ\n",
      "Ã€\n",
      "Äõ\n",
      "Äƒ\n",
      "Åè\n",
      "ÆØÇÃÅÅ\n",
      "ÇÇ\n",
      "ÇÍ\n",
      "ÊÊ\n",
      "ÊÊÊ\n",
      "ÊŒ\n",
      "ÌÄ\n",
      "Í^\n",
      "ÍÁÔ\n",
      "ÍÍÍ\n",
      "Íáõô\n",
      "Íî\n",
      "Íôýëá\n",
      "Í’\n",
      "Ñä\n",
      "ÓÏ\n",
      "ÔÏÕ\n",
      "ÔæÝó\n",
      "ÙË\n",
      "àâòîìî\n",
      "â¦€\n",
      "âè\n",
      "ã®\n",
      "ãå\n",
      "ãþâ\n",
      "äääääääääääää\n",
      "äë\n",
      "ä€\n",
      "åãî\n",
      "åè\n",
      "çý\n",
      "èòå\n",
      "êîãî\n",
      "êœ\n",
      "ëåä\n",
      "ìÝëïò\n",
      "ìàãàçè\n",
      "ìâ\n",
      "ìåôÜ\n",
      "ìê\n",
      "ìïðï\n",
      "ìïõ\n",
      "ìþ\n",
      "îáãùãÞò\n",
      "îç\n",
      "ðïó\n",
      "ðñÝðå\n",
      "òàçè\n",
      "óçò\n",
      "óìþ\n",
      "óôç\n",
      "ôÜ\n",
      "ôåô\n",
      "ôåôñáãù\n",
      "õîðà\n",
      "öü\n",
      "öِ\n",
      "ùì\n",
      "ùìåù\n",
      "ùú\n",
      "û’\n",
      "üê\n",
      "āč\n",
      "āṣā\n",
      "ćö\n",
      "đế\n",
      "ęź\n",
      "ġč\n",
      "ĥ\n",
      "Łęż\n",
      "ņģ\n",
      "ŘÁ\n",
      "śū\n",
      "Şö\n",
      "Şǝ\n",
      "şă\n",
      "şş\n",
      "šāḏ\n",
      "ššâ\n",
      "ūḥ\n",
      "ŮŽ\n",
      "Ƒ\n",
      "Ʃ\n",
      "ǃ\n",
      "ȇ\n",
      "ȚĂ\n",
      "Ȼ\n",
      "ȼ\n",
      "ɑ̜\n",
      "ɚ\n",
      "ɟ\n",
      "ɪʃ\n",
      "ʌŋ\n",
      "ʒɛ\n",
      "ʔ\n",
      "ʕ\n",
      "ʻō\n",
      "˃˃\n",
      "ˇˇˇ\n",
      "ˈɒ\n",
      "ˈɛ\n",
      "́=\n",
      "́Â\n",
      "́Ï\n",
      "́ç\n",
      "́ë\n",
      "̒\n",
      "̯\n",
      "ͤ\n",
      "ΐφοΠ\n",
      "ΑΓ\n",
      "ΑΕ\n",
      "ΑΘΗΝΑ\n",
      "ΑΡΤΙ\n",
      "Αρεοπαγίτου\n",
      "ΓΕ\n",
      "ΓΙΓΑΝΤΕΣ\n",
      "ΓΚ\n",
      "ΓΜΟ\n",
      "ΓΤΊ\n",
      "Γίγαντες\n",
      "Γεροσκήπου\n",
      "Γιώργος\n",
      "Γκαλερί\n",
      "Δαφνές\n",
      "Δημήτριος\n",
      "Διάσταση\n",
      "ΕΙΒ\n",
      "ΕΚ\n",
      "ΕΛΕΦΑΝΤΕΣ\n",
      "ΕΡ\n",
      "ΕΤΗ\n",
      "Εκπαιδευτικά\n",
      "Ελ\n",
      "Ελέφαντες\n",
      "Ελληνο\n",
      "Εξαιρετικό\n",
      "Επιτροπή\n",
      "Ζακύνθου\n",
      "Θεόδωρος\n",
      "Θεόφιλος\n",
      "Θῆβαι\n",
      "ΙΑΡΜΕ\n",
      "ΙΑΡΜΕΙ\n",
      "ΚΑΣΤΟΡΙΑΣ\n",
      "Λ\\\n",
      "Λήμνος\n",
      "Λεωφόρος\n",
      "Λη\n",
      "Μάκρη\n",
      "Μακεδονία\n",
      "Ματθαίος\n",
      "Μεσσαρά\n",
      "Μη\n",
      "Μιχαήλ\n",
      "Μόλυβδος\n",
      "ΝΤΟ\n",
      "Νεμέα\n",
      "ΟΗΓΜ\n",
      "ΟΚΑΝΑ\n",
      "Οι\n",
      "Οὕτως\n",
      "ΠΜ\n",
      "Πέρα\n",
      "Πειραιάς\n",
      "Πη\n",
      "Πλάκα\n",
      "Πτυχίο\n",
      "Πύργος\n",
      "ΡΤΈ\n",
      "Σητεία\n",
      "Σπυρίδων\n",
      "Σύνδεσμοι\n",
      "Σύνταγμα\n",
      "ΤΉΙ\n",
      "Τσίπουρο\n",
      "Τσικουδιά\n",
      "ΥΠΑ\n",
      "Υδρεύσεως\n",
      "Φασόλια\n",
      "ΧΠΙ\n",
      "Χριστιανικό\n",
      "Ψαρουδάκη\n",
      "άρθρο\n",
      "ίη\n",
      "α+β\n",
      "αντίγραφο\n",
      "ανώνυμη\n",
      "αυτον\n",
      "βάκτρον\n",
      "βγδ\n",
      "βραχύς\n",
      "για\n",
      "γλῶττα\n",
      "διδασκαλε\n",
      "ενδιάμεσου\n",
      "ενός\n",
      "επιφάνεια\n",
      "ετ\n",
      "ευθύνης\n",
      "ζύγον\n",
      "ημίγλυκος\n",
      "θ=\n",
      "θάνατος\n",
      "θεὸν\n",
      "θεὸς\n",
      "ι»\n",
      "ιο\n",
      "κέρας\n",
      "κλῆρος\n",
      "λεγεται\n",
      "λεπίς\n",
      "λύρα\n",
      "μÚ\n",
      "μάρτυς\n",
      "μρΑ\n",
      "ξηρός\n",
      "οβ\n",
      "οὐ\n",
      "παράδοση\n",
      "περιορισμένης\n",
      "πληροφοριες\n",
      "πρ\n",
      "προα\n",
      "προϊόντος\n",
      "ρεσις\n",
      "ρν\n",
      "σάρξ\n",
      "σαῦρος\n",
      "σημαντικες\n",
      "σιττη\n",
      "σουλτανίνα\n",
      "στόμα\n",
      "σύν\n",
      "σὺν\n",
      "τονωτικό\n",
      "τὸ\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print(tokenizer.decode([60000 + i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1a46b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"cls_token\": \"<s>\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87a6f025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [65001, 0], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<s>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7fa2bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65001"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5192e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "transformer = Transformer(encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0157aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(65002, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=65002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28ef003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5628b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# A function to encapsulate the training loop\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "  test_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    for batch in train_loader:\n",
    "      # move data to GPU (enc_input, enc_mask, translation)\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      enc_input = batch['input_ids']\n",
    "      enc_mask = batch['attention_mask']\n",
    "      targets = batch['labels']\n",
    "\n",
    "      # shift targets forwards to get decoder_input\n",
    "      dec_input = targets.clone().detach()\n",
    "      dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "      dec_input[:, 0] = 65_001\n",
    "\n",
    "      # also convert all -100 to pad token id\n",
    "      dec_input = dec_input.masked_fill(\n",
    "          dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "      # make decoder input mask\n",
    "      dec_mask = torch.ones_like(dec_input)\n",
    "      dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "      loss = criterion(outputs.transpose(2, 1), targets)\n",
    "        \n",
    "      # Backward and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    for batch in valid_loader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      enc_input = batch['input_ids']\n",
    "      enc_mask = batch['attention_mask']\n",
    "      targets = batch['labels']\n",
    "\n",
    "      # shift targets forwards to get decoder_input\n",
    "      dec_input = targets.clone().detach()\n",
    "      dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "      dec_input[:, 0] = 65_001\n",
    "\n",
    "      # change -100s to regular padding\n",
    "      dec_input = dec_input.masked_fill(\n",
    "          dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "      # make decoder input mask\n",
    "      dec_mask = torch.ones_like(dec_input)\n",
    "      dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "      outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "      loss = criterion(outputs.transpose(2, 1), targets)\n",
    "      test_loss.append(loss.item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    test_losses[it] = test_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "      Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "  \n",
    "  return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fe0c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 4.8972,       Test Loss: 3.8837, Duration: 0:00:18.651943\n",
      "Epoch 2/15, Train Loss: 3.5531,       Test Loss: 3.3775, Duration: 0:00:18.949467\n",
      "Epoch 3/15, Train Loss: 3.0734,       Test Loss: 3.0775, Duration: 0:00:21.209332\n",
      "Epoch 4/15, Train Loss: 2.7223,       Test Loss: 2.8326, Duration: 0:00:21.984545\n",
      "Epoch 5/15, Train Loss: 2.4304,       Test Loss: 2.6874, Duration: 0:00:21.848515\n",
      "Epoch 6/15, Train Loss: 2.1925,       Test Loss: 2.5589, Duration: 0:00:22.421354\n",
      "Epoch 7/15, Train Loss: 1.9998,       Test Loss: 2.4786, Duration: 0:00:19.781621\n",
      "Epoch 8/15, Train Loss: 1.8387,       Test Loss: 2.4408, Duration: 0:00:19.436924\n",
      "Epoch 9/15, Train Loss: 1.7099,       Test Loss: 2.4002, Duration: 0:00:18.441274\n",
      "Epoch 10/15, Train Loss: 1.5984,       Test Loss: 2.3828, Duration: 0:00:17.535151\n",
      "Epoch 11/15, Train Loss: 1.5056,       Test Loss: 2.3771, Duration: 0:00:17.470578\n",
      "Epoch 12/15, Train Loss: 1.4296,       Test Loss: 2.3712, Duration: 0:00:17.582448\n",
      "Epoch 13/15, Train Loss: 1.3553,       Test Loss: 2.3747, Duration: 0:00:17.644172\n",
      "Epoch 14/15, Train Loss: 1.2972,       Test Loss: 2.3852, Duration: 0:00:17.634244\n",
      "Epoch 15/15, Train Loss: 1.2483,       Test Loss: 2.3768, Duration: 0:00:17.612571\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = train(\n",
    "    transformer, criterion, optimizer, train_loader, valid_loader, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e37b9fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can I take a day off?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it out\n",
    "\n",
    "input_sentence = split['test'][10]['en']\n",
    "input_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e10df9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1283,   33,  273,    8,  502,  843,   21,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "enc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2361e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[65001,     0]]), 'attention_mask': tensor([[1, 1]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input_str = '<s>'\n",
    "\n",
    "dec_input = tokenizer(text_target=dec_input_str, return_tensors='pt')\n",
    "dec_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d31a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4291, -7.1860,  5.8436,  ..., -9.2412, -9.2837, -7.8867]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input.to(device)\n",
    "dec_input.to(device)\n",
    "output = transformer(\n",
    "    enc_input['input_ids'],\n",
    "    dec_input['input_ids'][:, :-1],\n",
    "    enc_input['attention_mask'],\n",
    "    dec_input['attention_mask'][:, :-1],\n",
    ")\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce0d244d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 65002])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape # N x T x V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e076f027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output = encoder(enc_input['input_ids'], enc_input['attention_mask'])\n",
    "enc_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06ee6539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 65002])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output = decoder(\n",
    "    enc_output,\n",
    "    dec_input['input_ids'][:, :-1],\n",
    "    enc_input['attention_mask'],\n",
    "    dec_input['attention_mask'][:, :-1],\n",
    ")\n",
    "dec_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34f5d3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(output, dec_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "685c26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input_ids = dec_input['input_ids'][:, :-1]\n",
    "dec_attn_mask = dec_input['attention_mask'][:, :-1]\n",
    "\n",
    "for _ in range(32):\n",
    "  dec_output = decoder(\n",
    "      enc_output,\n",
    "      dec_input_ids,\n",
    "      enc_input['attention_mask'],\n",
    "      dec_attn_mask,\n",
    "  )\n",
    "\n",
    "  # choose the best value (or sample)\n",
    "  prediction_id = torch.argmax(dec_output[:, -1, :], axis=-1)\n",
    "\n",
    "  # append to decoder input\n",
    "  dec_input_ids = torch.hstack((dec_input_ids, prediction_id.view(1, 1)))\n",
    "\n",
    "  # recreate mask\n",
    "  dec_attn_mask = torch.ones_like(dec_input_ids)\n",
    "\n",
    "  # exit when reach </s>\n",
    "  if prediction_id == 0:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80ad680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> ¿Puedo tomar un día para galletas?</s>'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dec_input_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db3f5fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Puedo tomarme un día libre?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split['test'][10]['es']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7839f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "  # get encoder output first\n",
    "  enc_input = tokenizer(input_sentence, return_tensors='pt').to(device)\n",
    "  enc_output = encoder(enc_input['input_ids'], enc_input['attention_mask'])\n",
    "\n",
    "  # setup initial decoder input\n",
    "  dec_input_ids = torch.tensor([[65_001]], device=device)\n",
    "  dec_attn_mask = torch.ones_like(dec_input_ids, device=device)\n",
    "\n",
    "  # now do the decoder loop\n",
    "  for _ in range(32):\n",
    "    dec_output = decoder(\n",
    "        enc_output,\n",
    "        dec_input_ids,\n",
    "        enc_input['attention_mask'],\n",
    "        dec_attn_mask,\n",
    "    )\n",
    "\n",
    "    # choose the best value (or sample)\n",
    "    prediction_id = torch.argmax(dec_output[:, -1, :], axis=-1)\n",
    "\n",
    "    # append to decoder input\n",
    "    dec_input_ids = torch.hstack((dec_input_ids, prediction_id.view(1, 1)))\n",
    "\n",
    "    # recreate mask\n",
    "    dec_attn_mask = torch.ones_like(dec_input_ids)\n",
    "\n",
    "    # exit when reach </s>\n",
    "    if prediction_id == 0:\n",
    "      break\n",
    "  \n",
    "  translation = tokenizer.decode(dec_input_ids[0, 1:])\n",
    "  print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f3276f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?</s>\n"
     ]
    }
   ],
   "source": [
    "translate(\"How are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508114c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
