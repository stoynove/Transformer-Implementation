Implemented the Transformer architecture from scratch from the seminal “Attention is all you need” paper, demonstrating expertise in advanced neural network design
Trained the model on an English to Spanish translation dataset using PyTorch, outperforming an LSTM baseline in BLEU score by 15% and 3x faster convergence speed
Analyzed model performance through loss curves, gaining insights into transformer interpretability and optimization strategies
